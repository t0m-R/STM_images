{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import omicronscala\n",
    "import spym\n",
    "import xarray\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.plt as plt\n",
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, filename):\n",
    "    with open('{}.pkl'.format(filename), 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open('{}.pkl'.format(filename), 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits https://github.com/aayushmnit/Deep_learning_explorations\n",
    "class Hook:\n",
    "    \"\"\"Create a hook on `m` with `hook_func`.\"\"\"\n",
    "    def __init__(self, m:nn.Module, hook_func:HookFunc, is_forward:bool=True, detach:bool=True):\n",
    "        self.hook_func,self.detach,self.stored = hook_func,detach,None\n",
    "        f = m.register_forward_hook if is_forward else m.register_backward_hook\n",
    "        self.hook = f(self.hook_fn)\n",
    "        self.removed = False\n",
    "\n",
    "    def hook_fn(self, module:nn.Module, input_tensor:Tensors, output_tensor:Tensors):\n",
    "        \"\"\"Applies `hook_func` to `module`, `input_tensor`, `output_tensor`.\"\"\"\n",
    "        if self.detach:\n",
    "            input_tensor  = (o.detach() for o in input_tensor ) if is_listy(input_tensor) else input_tensor.detach()\n",
    "            output_tensor = (o.detach() for o in output_tensor) if is_listy(output_tensor) else output_tensor.detach()\n",
    "        self.stored = self.hook_func(module, input_tensor, output_tensor)\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook from the model.\"\"\"\n",
    "        if not self.removed:\n",
    "            self.hook.remove()\n",
    "            self.removed=True\n",
    "\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.remove()\n",
    "        \n",
    "def get_output(output):\n",
    "    return output.flatten(1)\n",
    "\n",
    "def get_input(input_value):\n",
    "    return list(input_value)[0]\n",
    "\n",
    "def get_named_module_from_model(nn_model, name):\n",
    "    for n, m in nn_model.named_modules():\n",
    "        if n == name:\n",
    "            return m\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    \"\"\"create DataFrame for ImageList loader\"\"\"\n",
    "    df = load_pickle('clean_stm')\n",
    "    df['path'] = [ 'data/train/{}.png'.format(x) for x in df.index.values ]\n",
    "    random = []\n",
    "    for category in df['Categories'].unique():\n",
    "        if len(category.split(',')) > 1 or category == '':\n",
    "            random.append(category) \n",
    "    category_label = df.Categories.astype(\"category\").cat.codes\n",
    "    category = df['Categories']\n",
    "    df['category_label'] = category_label\n",
    "    df['dataset'] = 'train'\n",
    "    df['category'] = category\n",
    "    df['is_valid'] = [ True if x.Categories == 'mixed' else False for _,x in df.iterrows()]\n",
    "    df = df[['path', 'category_label', 'dataset', 'category', 'is_valid']][:]\n",
    "    return df\n",
    "\n",
    "def get_dict_category_labels(df):\n",
    "    \"\"\"create dictionary that maps categories with their labels\"\"\"\n",
    "    tmp = df.groupby(['category','category_label']).size().reset_index().rename(columns={0:'count'})\n",
    "    categories = tmp['category'].to_list()\n",
    "    labels = tmp['category_label'].to_list()\n",
    "    return dict(zip(labels,categories))\n",
    "\n",
    "def get_imgs_data(df, imgs_path):\n",
    "    imgs_list = ImageList.from_df(df=df, path=imgs_path, cols=['path']).split_from_df(col='is_valid').label_from_df(cols='category_label')\n",
    "    transforms = get_transforms()\n",
    "    imgs = data_source.transform(transforms, size=224).databunch(bs=32).normalize(imagenet_stats)\n",
    "    return imgs_list, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_features_df(layer, dataloader):\n",
    "    dict_features = {}\n",
    "\n",
    "    with Hook(layer, get_input, True, True) as hook:\n",
    "        for i, (xb, yb) in enumerate(dataloader):\n",
    "            bs = xb.shape[0]\n",
    "            if bs != 32:\n",
    "                img_ids = dataloader.items[-bs:]\n",
    "            else:\n",
    "                img_ids = dataloader.items[i*bs:(i+1)*bs]\n",
    "            model.eval()(xb)\n",
    "            features = hook.stored.cpu().numpy()\n",
    "            features = features.reshape(bs, -1)\n",
    "            for img_id, img_repr in zip(img_ids, features):\n",
    "                dict_features[img_id] = img_repr\n",
    "    \n",
    "    features_df = pd.DataFrame(dict_features.items(), columns=['img_path', 'img_repr'])\n",
    "    features_df['ID'] = [ x.split('/')[-1].split('.')[0] for x in features_df['img_path'] ]\n",
    "    features_df.set_index('ID', inplace=True)\n",
    "    features_df['label'] = [inference_data.classes[x] for x in inference_data.train_ds.y.items[0:features_df.shape[0]]]\n",
    "    features_df['label_id'] = inference_data.train_ds.y.items[0:features_df.shape[0]]\n",
    "    return features_df\n",
    "\n",
    "def torch_save(features_df, filename):\n",
    "    img_ft_df = features_df.copy()\n",
    "    img_ft_df['ID'] = [ x.split('/')[-1].split('.')[0] for x in features_df['img_path'] ]\n",
    "    img_ft_df.set_index('ID', inplace=True)\n",
    "    img_ft_df.drop(columns=['label', 'label_id'], inplace=True)\n",
    "    tmp = img_ft_df['img_repr'].to_numpy()\n",
    "    tmp = np.stack(tmp)\n",
    "    x = torch.from_numpy(tmp)\n",
    "    torch.save(x, '{}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe\n",
    "data_df = get_dataframe()\n",
    "\n",
    "# mapping categories -> labels\n",
    "dict_category_labels = get_dict_category_labels(data_df)\n",
    "save_pickle(dict_category_labels,'labels_dict')\n",
    "\n",
    "#data folder path\n",
    "images_path = Path('path_to_data_folder')\n",
    "\n",
    "# ImageLoader, Images\n",
    "data_source, data = get_imgs_data(data_df, images_path)\n",
    "\n",
    "#get resnet pretrained on imagenet\n",
    "learner = cnn_learner(data, models.resnet50, pretrained=True)\n",
    "model = learner.model\n",
    "\n",
    "#select layer for feature extraction\n",
    "linear_output_layer = get_named_module_from_model(model, '1.4')\n",
    "\n",
    "#prepare images and dataloader\n",
    "inference_data = data_source.transform(tmfs, size=224).databunch(bs=32).normalize(imagenet_stats)\n",
    "inference_dataloader = inference_data.train_dl.new(shuffle=False,drop_last=False)\n",
    "\n",
    "# get features df\n",
    "img_repr_df = get_img_features_df(linear_output_layer, inference_dataloader)\n",
    "save_pickle(img_repr_df, \"stm_features_df\")\n",
    "\n",
    "# simpler df\n",
    "df_features = img_repr_df[['img_repr', 'label']]\n",
    "save_pickle(df_features, \"stm_df_features\")\n",
    "torch_save(img_repr_df, 'S_features_resnet50_4096')\n",
    "\n",
    "# categories distribution\n",
    "len_dict = {}\n",
    "for k,v in dict_category_labels.items():\n",
    "    ldf = len(img_repr_df[img_repr_df['label']== k])/len(img_repr_df)\n",
    "    len_dict[v] = ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def get_similar_images(features_df, img_id, n=10):\n",
    "    img_id, features, label, _  = img_repr_df.loc[str(img_id)][:]\n",
    "    cosine_similarity = 1 - features_df['img_repr'].apply(lambda x: cosine(x, features))\n",
    "    similar_img_ids = np.argsort(cosine_similarity)[-n-1:-1][::-1]\n",
    "    return img_id, label, features_df.iloc[similar_img_ids]\n",
    "\n",
    "def get_similar_images_euclidean(features_df, img_id, n=10):\n",
    "    img_id, features, label, _  = img_repr_df.loc[str(img_id)][:]\n",
    "    similarity = features_df['img_repr'].apply(lambda x: euclidean(x, features))\n",
    "    similar_img_ids = np.argsort(similarity)[::][:n]\n",
    "    return img_id, label, features_df.iloc[similar_img_ids]\n",
    "\n",
    "def show_similar_images(features_df):\n",
    "    images = [open_image(img_id) for img_id in features_df['img_path']]\n",
    "    categories = [learner.data.train_ds.y.reconstruct(y) for y in features_df['label_id']]\n",
    "    return learner.data.show_xys(images, categories)\n",
    "\n",
    "def cosine_similarity_analysis(df, features_df, category_labels, trials=100, n_imgs=100):\n",
    "    analysis = {}\n",
    "    categories = df['label'].unique()\n",
    "    for category in categories:\n",
    "        start = time.time()\n",
    "        list_ids = []\n",
    "        dataframes = []\n",
    "        results_dict = {}\n",
    "        size = len(df[df['label']== category])\n",
    "        if size < trials:\n",
    "            print('skipping {}: requested {} samples out of {} images'.format(dict_category_labels[category], trials, size))\n",
    "            continue\n",
    "        tmp_df = df[df['label']== category].sample(trials)\n",
    "        for i, row in tmp_df.iterrows():\n",
    "            image, label, nn_features = get_similar_images(features_df, i, n_imgs)\n",
    "            list_ids.append(i)\n",
    "            dataframes.append(nn_features)\n",
    "            results = nn_features.groupby('label').size().reset_index(name='N imgs').sort_values(by='N imgs', ascending=False)\n",
    "            results['label'] = [ category_labels[x] for x in results['label'] ]\n",
    "            labels = results['label'].to_list()\n",
    "            n_imgs = results['N imgs'].to_list()\n",
    "            for l,n in zip(labels, n_imgs):\n",
    "                if l not in results_dict.keys():\n",
    "                    results_dict[l] = n\n",
    "                else:\n",
    "                    results_dict[l] += n\n",
    "        label = category_labels[category]\n",
    "        analysis[category] = {'ID': list_ids, 'dfs': dataframes, 'res': results_dict, 'label': label, 'len': size}\n",
    "        end = time.time()\n",
    "        print(f'{end - start} secs')\n",
    "    return analysis\n",
    "\n",
    "def euclidean_stats(df, trials=100, n_imgs=100):\n",
    "    analysis = {}\n",
    "    categories = df['label'].unique()\n",
    "    for category in categories:\n",
    "        start = time.time()\n",
    "        list_ids = []\n",
    "        dataframes = []\n",
    "        results_dict = {}\n",
    "        size = len(df[df['label']== category])\n",
    "        if size < trials:\n",
    "            print('skipping {}: requested {} samples out of {} images'.format(dict_category_labels[category], trials, size))\n",
    "            continue\n",
    "        tmp_df = df[df['label']== category].sample(trials)\n",
    "        for i, row in tmp_df.iterrows():\n",
    "            image, label, nn_features = get_similar_images_euclidean(features_df, i, n_imgs)\n",
    "            list_ids.append(i)\n",
    "            dataframes.append(nn_features)\n",
    "            results = nn_features.groupby('label').size().reset_index(name='N imgs').sort_values(by='N imgs', ascending=False)\n",
    "            results['label'] = [ category_labels[x] for x in results['label'] ]\n",
    "            labels = results['label'].to_list()\n",
    "            n_imgs = results['N imgs'].to_list()\n",
    "            for l,n in zip(labels, n_imgs):\n",
    "                if l not in results_dict.keys():\n",
    "                    results_dict[l] = n\n",
    "                else:\n",
    "                    results_dict[l] += n\n",
    "        label = category_labels[category]\n",
    "        analysis[category] = {'ID': list_ids, 'dfs': dataframes, 'res': results_dict, 'label': label, 'len': size}\n",
    "        end = time.time()\n",
    "        print(f'{end - start} secs')\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_date(new_date, start_date):\n",
    "    if new_date == start_date:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_same_offset(xoff,yoff,start_xoff, start_yoff,rounded=False):\n",
    "    if rounded: \n",
    "        if (int(xoff) == int(start_xoff)) and (int(yoff) == int(start_yoff)):\n",
    "            return True\n",
    "    else:\n",
    "        if (xoff == start_xoff) and (yoff == start_yoff):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_ids(df, df_ids, img_id, check_off=False, rounded=False):\n",
    "    good_imgs = [int(img_id)]\n",
    "    for i in df_ids.index.tolist():\n",
    "        x,y,d = df.loc[int(i)][[\"XOffset\",\"YOffset\",\"Date\"]].tolist()\n",
    "        is_good = True\n",
    "        for j in good_imgs:\n",
    "            xj,yj,dj=df.loc[int(j)][[\"XOffset\",\"YOffset\",\"Date\"]].tolist()\n",
    "            if is_same_date(d,dj):\n",
    "                if check_off:\n",
    "                    if is_same_offset(x,y,xj,yj,rounded):\n",
    "                        is_good = False\n",
    "                else:\n",
    "                    is_good = False\n",
    "        if is_good:\n",
    "            good_imgs.append(int(i))\n",
    "    if len(good_imgs)>25:\n",
    "        good_imgs = good_imgs[1:25]\n",
    "    else:\n",
    "        good_imgs = good_imgs[1:]\n",
    "    return good_imgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine(df, imgs_path, img_id, list_ids, fig_size=8, dpi=40):\n",
    "    start_img = get_img_by_id(df,imgs_path,int(img_id))\n",
    "    imgs = df.loc[df.index.intersection(list_ids)]\n",
    "    images = []\n",
    "    for i, image in imgs.iterrows():\n",
    "        try:\n",
    "            images.append(get_img(imgs_path, image))\n",
    "        except:\n",
    "            print(i)\n",
    "    plt.ioff()\n",
    "    rows=5\n",
    "    cols=5\n",
    "    figure, axs = plt.subplots(rows, cols, figsize=((fig_size*cols),(fig_size*rows)))\n",
    "    c = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if (i==2) and (j==2):\n",
    "                start_img[1].plot(ax=axs[i,j], cmap='afmhot', add_colorbar=False )\n",
    "                axs[i,j].set_title(r\"[{}] {} $\\bf{{{}}}$\".format(start_img[0]['Date'],\n",
    "                                                                 start_img[0]['TF0_Filename'],\n",
    "                                                                 start_img[0].name), fontsize=20)\n",
    "                for item in ([axs[i,j].xaxis.label, axs[i,j].yaxis.label] +\n",
    "                      axs[i,j].get_xticklabels() + axs[i,j].get_yticklabels()):\n",
    "                    item.set_fontsize(fsize*2)\n",
    "            else:\n",
    "                if c < len(images):\n",
    "                    images[c][1].plot(ax=axs[i,j], cmap='afmhot', add_colorbar=False )\n",
    "                    axs[i,j].set_title(r\"[{}] {} $\\bf{{{}}}$\".format(images[c][0]['Date'],\n",
    "                                                                     images[c][0]['TF0_Filename'],\n",
    "                                                                     images[c][0].name), fontsize=20)\n",
    "                    for item in ([axs[i,j].xaxis.label, axs[i,j].yaxis.label] +\n",
    "                          axs[i,j].get_xticklabels() + axs[i,j].get_yticklabels()):\n",
    "                        item.set_fontsize(fsize*2)\n",
    "                    c +=1\n",
    "    plt.tight_layout()\n",
    "    plt.draw()\n",
    "    Path('cosine/{}'.format(start_img[0]['Categories'])).mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig('cosine/{}/{}.png'.format(start_img[0]['Categories'],start_img[0].name), dpi=dpi)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(imgs_path, img):\n",
    "    \"\"\"get row of image from df, return list of [row,plot] for that image\"\"\"\n",
    "    file = img['ImageOriginalName']\n",
    "    ds = omicronscala.to_dataset(Path(imgs_path+file))\n",
    "    tf = ds.Z_Forward\n",
    "    tf.spym.plane()\n",
    "    tf.spym.align()\n",
    "    tf.spym.plane()\n",
    "    tf.spym.fixzero(to_mean=True)\n",
    "    return [img, tf]\n",
    "\n",
    "def get_img_by_id(df, imgs_path, img_id):\n",
    "    img = df.loc[img_id]\n",
    "    file = img.ImageOriginalName\n",
    "    ds = omicronscala.to_dataset(Path(imgs_path+file))\n",
    "    tf = ds.Z_Forward\n",
    "    tf.spym.plane()\n",
    "    tf.spym.align()\n",
    "    tf.spym.plane()\n",
    "    tf.spym.fixzero(to_mean=True)\n",
    "    return [img, tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_results(stats):\n",
    "    plt.ioff()\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    figure, axs = plt.subplots(rows, cols, figsize=(2+(10*cols),10*rows))\n",
    "    figure.suptitle('Cosine similarity for 100 trials of 100 images for each category', fontsize=36)\n",
    "    list_ids = list(stats.keys())\n",
    "    c = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if c < len(list_ids):\n",
    "                tmp_id = list_ids[c]\n",
    "                labels = list(stats[tmp_id]['res'].keys())\n",
    "                n_imgs = list(stats[tmp_id]['res'].values())\n",
    "                total = max(n_imgs)\n",
    "                n_imgs = [x/total for x in n_imgs]\n",
    "                label = stats[tmp_id]['label']\n",
    "                color = ['g' if x == label else 'b' for x in labels ]\n",
    "                axs[i,j].bar(labels, n_imgs, color=color)\n",
    "                axs[i,j].set_title('{}'.format(label), fontsize=28)\n",
    "                for item in ([axs[i,j].xaxis.label, axs[i,j].yaxis.label] +\n",
    "                      axs[i,j].get_xticklabels() + axs[i,j].get_yticklabels()):\n",
    "                    item.set_fontsize(16)\n",
    "                axs[i,j].set_ylabel('N images')\n",
    "                for tick in axs[i,j].get_xticklabels():\n",
    "                    tick.set_rotation(90)\n",
    "                c += 1\n",
    "            else:\n",
    "                axs[i,j].axis('off')\n",
    "                c += 1\n",
    "                \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.show()\n",
    "    fig.savefig('cosine_S_100x100.png')\n",
    "\n",
    "    \n",
    "def plot_analysis_results_norm(stats, prefix, trials, n_imgs, len_label_dict):\n",
    "    plt.ioff()\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    figure, axs = plt.subplots(rows, cols, figsize=(2+(10*cols),10*rows))\n",
    "    figure.suptitle('{} similarity for {} trials of {} images for each category'.format(prefix, trials, n_imgs), fontsize=36)\n",
    "    list_ids = list(stats.keys())\n",
    "    c = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if c < len(list_ids):\n",
    "                tmp_id = list_ids[c]\n",
    "                labels = list(stats[tmp_id]['res'].keys())\n",
    "                n_imgs = list(stats[tmp_id]['res'].values())\n",
    "                total = [ len_label_dict[x] for x in labels]\n",
    "                size = sum(n_imgs)\n",
    "                n_imgs = [x/size for x in n_imgs]\n",
    "                label = stats[tmp_id]['label']\n",
    "                color = ['g' if x == label else 'b' for x in labels ]\n",
    "                interval = np.arange(len(labels))\n",
    "                w = 0.3\n",
    "                axs[i,j].bar(interval+0.0, n_imgs, w, color=color)\n",
    "                axs[i,j].bar(interval+0.3, total, w, color='r')\n",
    "                axs[i,j].set_xticks(interval)\n",
    "                axs[i,j].set_xticklabels(labels)\n",
    "                axs[i,j].set_title('{}'.format(label), fontsize=28)\n",
    "                for item in ([axs[i,j].xaxis.label, axs[i,j].yaxis.label] +\n",
    "                      axs[i,j].get_xticklabels() + axs[i,j].get_yticklabels()):\n",
    "                    item.set_fontsize(16)\n",
    "                axs[i,j].set_ylabel('N images')\n",
    "                for tick in axs[i,j].get_xticklabels():\n",
    "                    tick.set_rotation(90)\n",
    "                c += 1\n",
    "            else:\n",
    "                axs[i,j].axis('off')\n",
    "                c += 1\n",
    "                \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.show()\n",
    "    fig.savefig('{}_S_{}x{}.png'.format(prefix, trials, n_imgs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to original imgs\n",
    "path = 'path_to_images'\n",
    "\n",
    "#load stm metadata df\n",
    "stm = load_pickle('clean_stm')\n",
    "\n",
    "# good images manually selected\n",
    "goods_dict = { \"N_Gr_Ni111\": [87980, 87931, 88019, 87795, 84551, 84568, 87048, 83206, \n",
    "87912, 87774], \"Gr_Ni111\" : [85804, 83502, 83080, 83018, 83005, 82729, 82061, 50736, 50701, \n",
    "49062], \"Gr_Ni100\": [77857, 77795, 77809, 77690, 77696, 79863, 77649, 77626, 79779, \n",
    "79729],\"NFFA_ID617\": [86169, 86687, 86374, 83699, 83734, 84700, 83880, 84133, \n",
    "85763, 85800]}\n",
    "\n",
    "# similarity search with filtering for each image\n",
    "for k,v in goods_dict.items():\n",
    "    print('\\n{}'.format(k))\n",
    "    for image_id in v:\n",
    "        print('\\t{}'.format(image_id))\n",
    "        _, _, ids = get_similar_images(img_repr_df, image_id, 250)\n",
    "        nn_ids = filter_ids(stm, ids, image_id, check_off=True, rounded=True)\n",
    "        plot_cosine(stm, imgs_path, image_id, ID, nn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single image similarity example\n",
    "base_image, base_label, similar_images_df = get_similar_images_euclidean(img_repr_df, 44148, 100)\n",
    "print(base_label)\n",
    "print(base_image)\n",
    "open_image(base_image)\n",
    "show_similar_images(similar_images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features validation by statistical analysis on extracted images from similarity search\n",
    "\n",
    "stats2 = cosine_similarity_analysis(img_repr_df, 500, 20)\n",
    "plot_analysis_results_norm(stats2, 'cosine',500, 20, len_dict)\n",
    "save_pickle(stats2, 'cosine_500_20')\n",
    "\n",
    "stats3 = euclidean_stats(img_repr_df, 100, 100)\n",
    "plot_analysis_results_norm(stats3, 'euclidean', 100, 100, len_dict)\n",
    "save_pickle(stats3, 'euclidean_100_100')\n",
    "\n",
    "stats4 = euclidean_stats(img_repr_df, 500, 20)\n",
    "plot_analysis_results_norm(stats4, 'euclidean', 500, 20, len_dict)\n",
    "save_pickle(stats4, 'euclidean_500_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_intrinsic_dimension (data_array):\n",
    "    n = data_array.shape[0]\n",
    "    nn = NearestNeighbors(n_neighbors=3, algorithm='kd_tree', n_jobs=-1).fit(data_array)\n",
    "    nn_distances, nn_indices = nn.kneighbors(data_array)\n",
    "    mu = nn_distances[:,2] / nn_distances[:,1]\n",
    "    i_sorted = np.argsort(mu)\n",
    "    f_emp = np.zeros(n, dtype=float)\n",
    "    f_emp[i_sorted] = [i /n for i in range(n)]\n",
    "    x = np.log(mu).reshape(-1,1)\n",
    "    y = -np.log(1. - F_emp).reshape(-1,1)\n",
    "    l = LinearRegression(fit_intercept=False, n_jobs=1).fit(x,y)\n",
    "    return l.coef_[0,0]\n",
    "\n",
    "def plot_components(data_array, nn_model, images=None, axs=None,\n",
    "                    thumb_frac=0.05, colormap='gray'):\n",
    "    axs = axs or plt.gca()\n",
    "    proj = nn_model.fit_transform(x)\n",
    "    axs.plot(proj[:, 0], proj[:, 1], '.k')\n",
    "    if images is not None:\n",
    "        min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\n",
    "        shown_images = np.array([2 * proj.max(0)])\n",
    "        for i in range(data_array.shape[0]):\n",
    "            dist = np.sum((proj[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < min_dist_2:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.vstack([shown_images, proj[i]])\n",
    "            image_box = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(images[i], cmap=colormap),\n",
    "                                      proj[i])\n",
    "            ax.add_artist(image_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "mod = Isomap(n_components=2)\n",
    "xx = torch.load('S_features_resnet50_4096')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_components(xx, mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = xx\n",
    "distorsions = []\n",
    "for k in range(2, 20):\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    k_means.fit(X)\n",
    "    distorsions.append(k_means.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 20), distorsions)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy2=np.diff(distorsions,n=2)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 18), dy2)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy2_log=np.diff(np.log(distorsions),n=2)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 18), dy2)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stm",
   "language": "python",
   "name": "stm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}